# 拥抱脸:解决文本分类问题的基本任务教程

> 原文：<https://www.stxnext.com/blog/hugging-face-tutorial/>

 [](/blog/top-python-nlp-libraries/)自然语言处理是 [AI](https://www.stxnext.com/blog/will-artificial-intelligence-replace-developers/) 的领先领域之一，涉及使计算机能够理解人类语言。

然而，为了实现 NLP 的全部潜力，您需要能够使用最新的 Transformer 架构，这是一种采用自我关注机制的深度学习模型，对输入数据的每个部分的重要性进行不同的加权。

这似乎是一项艰巨的任务，但是有很多工具可以使 NLP 模型的工作变得更加简单。其中一个是拥抱脸，一个值得特别一提的库。

拥抱脸可以让你缩短与最新的 NLP 解决方案和技术的距离，同时也可以从中获得一些乐趣。虽然库看起来是一个相当复杂的工具，但其实根本不是那么回事。事实上，它在 NLP 工作中对初学者和更高级的用户都非常有用。

我们将从获得现代自然语言处理体系结构和思想的理论知识开始我们的文章，然后转移到一个实际的任务，我们将使用拥抱脸和我们所学的所有理论元素来构建一个文本分类器。请继续阅读！ 

#### 注意机制与迁移学习

近几年在 [机器学习](/services/machine-learning/) 领域出现了很多极其有意义的模型，自然语言处理(NLP)领域也不例外。

它始于 2017 年，引入了新的 Transformer 架构，实现了高度复杂和强大的 NLP 模型。由于注意力机制，他们没有像以前的 RNN 模式那样缺乏训练并行性。

然后，2018 年初，ULMFiT NLP 模型问世。它受益于迁移学习的思想，这反过来使它有可能使用相对少量的标记数据来微调 NLP 模型。

这两种努力都有助于发现现代 NLP 中最重要和最先进的两个模型:伯特和 GPT。

#### 伯特是什么？

**BERT 模型是 Transformer 架构的编码器部分，使用屏蔽语言建模(MLM)和下一句预测目标(NSP)进行训练。**

MLM 在训练期间强迫模型从输入向量中猜测随机隐藏的单词。一个期望的副作用是模型学习输入句子的隐藏含义的能力，因为为了猜测隐藏的单词，模型必须“浏览”整个句子。

最初的 BERT 最初是在一般文本的语料库上训练的。从那以后，无数的 BERT 变体被创造出来，但是它们都基于相同的基本思想。

#### 什么是 GPT？

GPT 模型是变压器架构中解码器的一部分。一方面，和 BERT 一样，它已经在一般文本的语料库上进行了预处理，所以它是 ULMFiT 迁移学习的继承者。另一方面，它拥有以注意力机制为主导的变压器模型的所有优点。

这是一个“真正的”语言模型，因为它的工作是根据给定的起始上下文预测下一个单词。出于这个原因，它被用于 BERT 以外的任务，比如文本生成和摘要，我们将在本文后面讨论。

类似于伯特模型，GPT 已经看到了后继者，如 GPT-2 和最近推出的 GPT-3。尽管它们在参数数量上有所不同，但主要的架构原则是相同的。

#### 什么是抱脸？抱脸是做什么的？

继伯特和 GPT 之后，又有许多新的机器人被开发出来解决各种下游任务。也就是说，任何这样的发明都是在没有特别关注代码标准化的情况下发布的，所以能够让这些模型适应新任务并不容易。

为了解决这个问题，并使模型的体系结构和它们对进一步任务的适用性标准化，拥抱人脸库被引入。

在最开始，它只是一个初创公司，提供了一个基于流行的 Transformer 模型的实现的开源存储库。现在，它是一个超过 50，000 个预训练模型的优秀来源，具有源代码和致力于解决各种下游任务的头部。

拥抱人脸库包括重要的自然语言处理和图像处理数据集，可以立即使用。它还提供了运行模型所必需的外围设备，比如令牌化器和指标的实现，以及必要的文档、使用示例，甚至是针对各种任务的专用训练器。

##### 拥抱脸中枢

现在，大多数 NLP 任务都可以通过使用来自拥抱脸库的现代模型来解决，拥抱脸库为模型和所有外围设备的实现提供了标准化的开源代码。

但这并不是拥抱脸提供的一切。它还标准化了典型的 NLP 管道，可以让您通过定义明确的步骤来完成在特定数据集上微调模型的整个过程。

##### 加载数据集

第一步是从超过 5，000 个可用的集合中加载数据集。当然，我们可以使用自己的语料库，其与库的详细联系将在本文后面讨论。

该库提供的数据集的一个关键特征是，它们可以并入 [Pandas 或 NumPy 库](/blog/most-popular-python-scientific-libraries/) ，这在如今的 [数据科学](/services/data-engineering/) 和机器学习社区中非常重要。

##### 对文本进行标记

一旦准备好文本语料库，就必须以某种方式将其输入到模型中。因此，我们需要将我们的文本转换成数字表示，就像我们在初始训练过程中所做的那样。要做到这一点，我们只需用模型标记器初始化提供文本的过程，并将整个语料库输入其中。

大多数标记化器都是用 RUST 编程语言编写的，所以标记化过程不会花太多时间。相反，由于拥抱脸库提供的标准化，我们可以以相同的方式使用不同的标记器，而不管它们内部是如何工作的。

##### 微调模型

模型微调是一个过程，在该过程中，预训练模型被馈入特定于领域的数据，以使其在专用环境中更好地工作。不同的任务需要不同的调优策略。

一些包括在模型上应用一个特殊的头部，并根据一个新的目标进行训练，比如交叉熵。在其他情况下，模型通过根据与预训练相同的目标进行训练来查看新数据就足够了。

无论微调的类型如何，必要的头部和目标都由具有高级标准化的库提供，这意味着知道我们想要执行的任务后，我们选择一个具有适当头部的预定义模型，其余的工作对于大多数任务都以相同的方式完成。

#### 如何用拥抱脸解决文本分类问题

我们从 NLP 中最常见的任务开始探索拥抱脸库:文本分类。它的流行与它可以用于的广泛任务有关。

例如，垃圾邮件过滤器基本上是使用一种基于邮件内容将邮件分类为垃圾邮件的机制构建的。

一种类似的机制可以用来自动检测仇恨言论，这可以确定管理员应该注意的危险的 Twitter 或脸书帖子。

另一个例子可能是一个扫描整个互联网以检查客户对产品的总体意见的系统。

在我们的例子中，我们将试着建立一个系统，可以根据主题对新闻文章进行分类:世界、体育、商业和科技。为了做到这一点，我们将完全依靠拥抱脸库。

首先，我们必须从 Hugging Face Hub 加载数据集，然后使用这个库实现的标记化器。

下一步是将预定义模型的权重从中枢加载到相应的模型类中，该模型类具有分类任务的头。

最后，我们将使用培训师类来微调和评估模型。

##### 1.加载数据集

我们将要使用的数据集在拥抱脸中枢*中被命名为*“ag _ news”*。*为了加载它，我们必须简单地从*数据集*库中导入 *load_dataset* 方法，然后用正确的数据集名称初始化它: